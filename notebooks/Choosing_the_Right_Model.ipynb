{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOB4GHEqnQ7Pr0zZQ0GpfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/novacellus/workshop_llms_25/blob/main/notebooks/Choosing_the_Right_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the environment / Python Primer / Colab Interface"
      ],
      "metadata": {
        "id": "JI8E74jRO_4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UNQNLKvidYHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897c7196-ef1a-492a-e09c-09873331b4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ancient Roman literature occasionally employed hateful language to mock or condemn enemies, exemplifying the era's use of rhetoric as a means of social and political expression.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY') # read in the secret API key to a variable\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# contact the API by specifying the model you want to work with and your input\n",
        "# the response will be stored in a variable called... \"response\"\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    input=\"Write one sentence about the use of hateful language in ancient Roman literature.\"\n",
        ")\n",
        "\n",
        "# print the response by accessing the output_text key in the response\n",
        "# print() is a built-in function and takes as argument the THING we wish to print\n",
        "# response is a complex object storing a bunch of data, we can access it by using dot-notation:\n",
        "# VARIABLE.SOME_KEY.ANOTHER_KEY ...\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR TURN: Print the entire response below and analyze its components\n",
        "print(response)"
      ],
      "metadata": {
        "id": "xF0rTnEANz1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9621fe28-b8dd-448b-a29f-aebc097cf55e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response(id='resp_68332ec4eff08191b66d578a315e86ee048928b11d19f549', created_at=1748184772.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_68332ec551d48191b80287c69b953f02048928b11d19f549', content=[ResponseOutputText(annotations=[], text=\"Ancient Roman literature occasionally employed hateful language to mock or condemn enemies, exemplifying the era's use of rhetoric as a means of social and political expression.\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=21, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=53), user=None, background=False, store=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR TURN: Print the usage information about this call. It's stored under \"usage\" key\n",
        "print(response.usage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpU-uHr1Q1hh",
        "outputId": "56a165fa-dd6b-4128-d05f-2ec80734b5a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResponseUsage(input_tokens=21, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining API query function\n",
        "During this hands on, we will contact OpenAI API numerous times sending it a prompt, example, and waiting for response. Since we don't want to repeat our code every time, we will define a function `query_models` which will be responsible for sending the call and other simple functionalities (e.g. computing the cost of our calls)."
      ],
      "metadata": {
        "id": "13BFwU5PQzbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def query_models(models, prompt, system_prompt=\"You are a helpful assistant.\", max_tokens=250):\n",
        "    \"\"\"\n",
        "    Query multiple models with the same prompt and track token usage and costs\n",
        "\n",
        "    Args:\n",
        "        models (list): List of model names to use\n",
        "        prompt (str): The user prompt\n",
        "        system_prompt (str): The system prompt\n",
        "        max_tokens (int): Maximum response length\n",
        "\n",
        "    Returns:\n",
        "        dict: Results with model names as keys and responses as values\n",
        "    \"\"\"\n",
        "    # Print the prompt\n",
        "    print(f\"PROMPT:\\n{prompt}\\n\")\n",
        "    print(f\"SYSTEM:\\n{system_prompt}\\n\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model in models:\n",
        "        print(f\"\\nQuerying {model}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.2  # Lower temperature for more consistent results\n",
        "            )\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            response_text = response.choices[0].message.content\n",
        "\n",
        "            # Get token usage\n",
        "            prompt_tokens = response.usage.prompt_tokens\n",
        "            completion_tokens = response.usage.completion_tokens\n",
        "            total_tokens = response.usage.total_tokens\n",
        "\n",
        "            # Calculate costs\n",
        "            if model in MODEL_PRICING:\n",
        "                input_cost = (prompt_tokens / 1000) * MODEL_PRICING[model][\"input\"]\n",
        "                output_cost = (completion_tokens / 1000) * MODEL_PRICING[model][\"output\"]\n",
        "                total_cost = input_cost + output_cost\n",
        "                cost_info = f\"Cost: ${total_cost:.6f} (Input: ${input_cost:.6f}, Output: ${output_cost:.6f})\"\n",
        "            else:\n",
        "                cost_info = \"Cost: Unknown (pricing not available for this model)\"\n",
        "\n",
        "            # Print model name, token usage, cost, and response\n",
        "            print(f\"\\nMODEL: {model} (took {elapsed_time:.2f} seconds)\")\n",
        "            print(f\"RESPONSE:\\n{response_text}\\n\\n\")\n",
        "            print(f\"TOKENS: {total_tokens} total (Prompt: {prompt_tokens}, Completion: {completion_tokens})\")\n",
        "            print(f\"{cost_info}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Store results including token usage and cost\n",
        "            results[model] = {\n",
        "                \"response\": response_text,\n",
        "                \"prompt_tokens\": prompt_tokens,\n",
        "                \"completion_tokens\": completion_tokens,\n",
        "                \"total_tokens\": total_tokens,\n",
        "                \"time_seconds\": elapsed_time,\n",
        "                \"estimated_cost\": total_cost if model in MODEL_PRICING else None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nMODEL: {model}\")\n",
        "            print(f\"ERROR: {str(e)}\")\n",
        "            print(\"-\" * 50)\n",
        "            results[model] = {\"error\": str(e)}\n",
        "\n",
        "    # Print a summary of token usage and costs\n",
        "    print(\"\\nSUMMARY:\")\n",
        "    for model, result in results.items():\n",
        "        if \"error\" not in result:\n",
        "            cost = result.get(\"estimated_cost\")\n",
        "            cost_str = f\"${cost:.6f}\" if cost is not None else \"Unknown\"\n",
        "            print(f\"{model}: {result['total_tokens']} tokens, Cost: {cost_str}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "iGHgRWcfoMOd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice. Sentiment analysis with OpenAI models"
      ],
      "metadata": {
        "id": "MynXKeICSfpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by defining the models we'll be using in this exercise and their current pricing."
      ],
      "metadata": {
        "id": "UOOjgd0PSmVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected models\n",
        "MODELS = [\n",
        "    \"gpt-4.1\",      # \"Flagship GPT model for complex tasks\": $2.00 / $8.00\n",
        "    \"gpt-4.1-nano\",    # \"Fastest, most cost-effective GPT-4.1 model\": $0.10 / $0.40\n",
        "]\n",
        "MODEL_PRICING = {\n",
        "    \"gpt-4.1\": {\"input\": 0.02, \"output\": 0.08},\n",
        "    \"gpt-4.1-nano\": {\"input\": 0.0001, \"output\": 0.0008}\n",
        "}"
      ],
      "metadata": {
        "id": "Up-KFBEfKRFC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to define variables which store our prompts and data we wish to classify."
      ],
      "metadata": {
        "id": "nZfZxEImS0Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texts for analysis\n",
        "sentence_la = \"Quo usque tandem abutere, Catilina, patientia nostra?\"\n",
        "sentence_en = \"How long, Catiline, will you abuse our patience?\"\n",
        "\n",
        "# Task prompts\n",
        "sentiment_prompt = f\"Classify the sentiment of this Latin text as positive, negative, or neutral.\\n\\n{sentence_la}\"\n",
        "sentiment_system = \"You are an expert in classical Latin literature, rhetoric, and linguistics.\"\n",
        "\n",
        "# Query the models\n",
        "results = query_models(MODELS, sentiment_prompt, sentiment_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZdKSRfHKHfH",
        "outputId": "3b798801-012f-467b-c95f-0b52b810eeb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT:\n",
            "Classify the sentiment of this Latin text as positive, negative, or neutral.\n",
            "\n",
            "Quo usque tandem abutere, Catilina, patientia nostra?\n",
            "\n",
            "SYSTEM:\n",
            "You are an expert in classical Latin literature, rhetoric, and linguistics.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Querying gpt-4.1...\n",
            "\n",
            "MODEL: gpt-4.1 (took 2.16 seconds)\n",
            "RESPONSE:\n",
            "The sentiment of the Latin text **\"Quo usque tandem abutere, Catilina, patientia nostra?\"** is **negative**.\n",
            "\n",
            "**Reasoning:**  \n",
            "This famous line, spoken by Cicero in his First Catilinarian Oration, is an accusatory and exasperated question directed at Catiline. Cicero is expressing frustration and indignation at Catiline's continued abuse of the Senate's patience. The tone is reproachful and critical, indicating a negative sentiment.\n",
            "\n",
            "\n",
            "TOKENS: 160 total (Prompt: 59, Completion: 101)\n",
            "Cost: $0.009260 (Input: $0.001180, Output: $0.008080)\n",
            "--------------------------------------------------\n",
            "\n",
            "Querying gpt-4.1-nano...\n",
            "\n",
            "MODEL: gpt-4.1-nano (took 0.89 seconds)\n",
            "RESPONSE:\n",
            "The sentiment of the Latin text \"Quo usque tandem abutere, Catilina, patientia nostra?\" is negative. It expresses frustration and reproach toward Catiline, indicating disapproval and concern.\n",
            "\n",
            "\n",
            "TOKENS: 102 total (Prompt: 59, Completion: 43)\n",
            "Cost: $0.000040 (Input: $0.000006, Output: $0.000034)\n",
            "--------------------------------------------------\n",
            "\n",
            "SUMMARY:\n",
            "gpt-4.1: 160 tokens, Cost: $0.009260\n",
            "gpt-4.1-nano: 102 tokens, Cost: $0.000040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR TURN: Modify the code to evaluate the English translation of the Cicero's phrase. Compare results.\n",
        "# Texts for analysis\n",
        "sentence_la = \"Quo usque tandem abutere, Catilina, patientia nostra?\"\n",
        "sentence_en = \"How long, Catiline, will you abuse our patience?\"\n",
        "\n",
        "# Sample task prompts\n",
        "sentiment_prompt = f\"Classify the sentiment of this Latin text as positive, negative, or neutral.\\n\\n{sentence_en}\"\n",
        "sentiment_system = \"You are an expert in classical Latin literature, rhetoric, and linguistics.\"\n",
        "\n",
        "# Example call\n",
        "results = query_models(MODELS, sentiment_prompt, sentiment_system)"
      ],
      "metadata": {
        "id": "Po-PVBxmHlBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f102cfe0-8540-4cf2-849a-ff6fe859fad5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROMPT:\n",
            "Classify the sentiment of this Latin text as positive, negative, or neutral.\n",
            "\n",
            "How long, Catiline, will you abuse our patience?\n",
            "\n",
            "SYSTEM:\n",
            "You are an expert in classical Latin literature, rhetoric, and linguistics.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Querying gpt-4.1...\n",
            "\n",
            "MODEL: gpt-4.1 (took 1.14 seconds)\n",
            "RESPONSE:\n",
            "The sentiment of the Latin text \"How long, Catiline, will you abuse our patience?\" (Latin: *Quousque tandem abutere, Catilina, patientia nostra?*) is **negative**.\n",
            "\n",
            "This sentence expresses frustration, accusation, and exasperation toward Catiline, indicating a negative sentiment.\n",
            "\n",
            "\n",
            "TOKENS: 119 total (Prompt: 54, Completion: 65)\n",
            "Cost: $0.006280 (Input: $0.001080, Output: $0.005200)\n",
            "--------------------------------------------------\n",
            "\n",
            "Querying gpt-4.1-nano...\n",
            "\n",
            "MODEL: gpt-4.1-nano (took 0.57 seconds)\n",
            "RESPONSE:\n",
            "The sentiment of the Latin text \"How long, Catiline, will you abuse our patience?\" is negative.\n",
            "\n",
            "\n",
            "TOKENS: 76 total (Prompt: 54, Completion: 22)\n",
            "Cost: $0.000023 (Input: $0.000005, Output: $0.000018)\n",
            "--------------------------------------------------\n",
            "\n",
            "SUMMARY:\n",
            "gpt-4.1: 119 tokens, Cost: $0.006280\n",
            "gpt-4.1-nano: 76 tokens, Cost: $0.000023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfYcsAbw72j3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}